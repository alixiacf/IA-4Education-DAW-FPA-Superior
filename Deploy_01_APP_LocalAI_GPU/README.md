AplicaciÃ³n de Chat Gemma2 con Soporte para GPU ğŸ’¬âš¡

Este proyecto implementa una interfaz de chat basada en la web para interactuar con el modelo de lenguaje Gemma2 de Google, ejecutado localmente a travÃ©s de Ollama con aceleraciÃ³n GPU.

DescripciÃ³n General
La aplicaciÃ³n consta de tres componentes principales:

Servicio Ollama: Ejecuta el modelo de IA Gemma2 con aceleraciÃ³n GPU.
Backend en Python: Proporciona una interfaz web usando Gradio y se comunica con Ollama.
Infraestructura Docker: Orquesta los componentes con la red adecuada y el paso de GPU.

CaracterÃ­sticas âœ¨
Despliegue local del modelo de lenguaje Gemma2.
AceleraciÃ³n GPU para una inferencia mÃ¡s rÃ¡pida âš¡.
Interfaz de chat web simple y fÃ¡cil de usar ğŸ’».
ConfiguraciÃ³n en contenedor para un despliegue sencillo ğŸš€.

Requisitos ğŸ”§
Docker y Docker Compose.
GPU NVIDIA con soporte CUDA ğŸ–¥ï¸.
NVIDIA Container Toolkit instalado.
Se recomienda tener al menos 8 GB de VRAM en la GPU para un rendimiento Ã³ptimo.

Arquitectura ğŸ—ï¸
ConfiguraciÃ³n de Docker Compose
La aplicaciÃ³n se orquesta utilizando Docker Compose con los siguientes servicios:

Servicio Ollama:

Basado en la imagen ollama/ollama.
Proceso de construcciÃ³n personalizado que incluye el script run.sh.
ConfiguraciÃ³n de paso de GPU para la aceleraciÃ³n ğŸš€.
Descarga automÃ¡tica del modelo Gemma2 al iniciar.
Expone el puerto 11434 para acceso a la API.
AplicaciÃ³n Web:

Backend en Python utilizando Gradio ğŸ.
Proporciona una interfaz de chat amigable para el usuario.
Se comunica con Ollama a travÃ©s de su API ğŸ”—.
Expone el puerto 7860 para acceso web.

ConfiguraciÃ³n de Red ğŸŒ
Ambos servicios se comunican a travÃ©s de una red puente llamada mynetwork, con la aplicaciÃ³n web configurada para depender del servicio Ollama, asegurando que se inicien en el orden correcto.

Funcionamiento âš™ï¸
El contenedor de Ollama se inicia y ejecuta el script run.sh, que:

Inicia el servicio Ollama.
Descarga automÃ¡ticamente el modelo Gemma2 ğŸ“¥.
Mantiene el contenedor en ejecuciÃ³n.
El contenedor del backend en Python:

Proporciona una interfaz web a travÃ©s de Gradio.
Recibe la entrada del usuario y la envÃ­a a la API de Ollama.
Muestra las respuestas de la API de vuelta en la interfaz de usuario.
La aceleraciÃ³n GPU:

Los controladores NVIDIA se pasan al contenedor Ollama ğŸ®.
Habilita la aceleraciÃ³n por hardware para la inferencia.
Mejora significativamente los tiempos de respuesta â±ï¸.

CÃ³mo Ejecutarlo ğŸš€
Clona este repositorio.
AsegÃºrate de que el NVIDIA Container Toolkit estÃ© instalado correctamente.
Ejecuta docker-compose up -d.
Accede a la interfaz de chat en http://localhost:7860.

SoluciÃ³n de Problemas ğŸ› ï¸
Si encuentras errores relacionados con la GPU, verifica que el NVIDIA Container Toolkit estÃ© instalado correctamente.
Si hay problemas al descargar el modelo, revisa tu conexiÃ³n a Internet ğŸŒ y el espacio disponible en disco.
Si la interfaz web no puede conectarse a Ollama, asegÃºrate de que ambos contenedores estÃ©n en ejecuciÃ³n correctamente.

Licencia ğŸ“„
Este proyecto es de cÃ³digo abierto y estÃ¡ disponible para uso educativo y personal.
