AplicaciÃ³n de Chat Gemma2 con Soporte para CPU

Este proyecto implementa una aplicaciÃ³n web de chat que permite interactuar con el modelo de lenguaje Gemma2 de Google, ejecutado localmente a travÃ©s de Ollama sin necesidad de una GPU. EstÃ¡ diseÃ±ado para proporcionar una experiencia de uso fluida y accesible en cualquier sistema que cumpla con los requisitos mÃ­nimos.

ğŸ“Œ CaracterÃ­sticas Principales
âœ… ImplementaciÃ³n local del modelo Gemma2, sin depender de servidores en la nube.
âœ… EjecuciÃ³n basada en CPU, ideal para sistemas sin aceleraciÃ³n por GPU.
âœ… Interfaz web intuitiva basada en Gradio, accesible desde el navegador.
âœ… ConfiguraciÃ³n con Docker, lo que simplifica la instalaciÃ³n y ejecuciÃ³n.
âœ… Arquitectura modular, con separaciÃ³n clara entre backend y el servicio de IA.
âœ… AutomatizaciÃ³n de despliegue, asegurando una instalaciÃ³n sencilla con un solo comando.

ğŸ› ï¸ Requisitos Previos
Para ejecutar correctamente la aplicaciÃ³n, asegÃºrate de contar con lo siguiente en tu sistema:
Docker (Ãºltima versiÃ³n recomendada).
Docker Compose (para la gestiÃ³n de los contenedores).
ConexiÃ³n a internet para la descarga inicial del modelo.
Al menos 4 GB de RAM disponibles para una ejecuciÃ³n Ã³ptima en CPU.

ğŸ“‚ Estructura del Proyecto
El proyecto estÃ¡ organizado en los siguientes componentes:

ğŸ“ backend/ â†’ Contiene el cÃ³digo fuente de la API y la interfaz web.
ğŸ“ config/ â†’ Archivos de configuraciÃ³n y scripts necesarios para la ejecuciÃ³n.
ğŸ“ docker/ â†’ ConfiguraciÃ³n y archivos para la gestiÃ³n de contenedores.
ğŸ“ models/ â†’ Modelos de lenguaje descargados y almacenados localmente.
ğŸ“œ docker-compose.yml â†’ Archivo de configuraciÃ³n para ejecutar los contenedores.
ğŸ“œ README.md â†’ DocumentaciÃ³n del proyecto.

âš™ï¸ Arquitectura del Sistema
La aplicaciÃ³n utiliza Docker Compose para orquestar los diferentes servicios:

1ï¸âƒ£ Servicio de Ollama (Inteligencia Artificial)
Basado en la imagen oficial ollama/ollama.
Descarga y ejecuta automÃ¡ticamente el modelo Gemma2 al iniciarse.
Expone la API en el puerto 11434.
Contiene un script run.sh que gestiona la inicializaciÃ³n.

2ï¸âƒ£ Backend y WebApp
Implementado en Python utilizando Gradio para la interfaz de usuario.
ActÃºa como puente entre el usuario y el servicio Ollama.
Expone la aplicaciÃ³n web en el puerto 7860.

3ï¸âƒ£ Red y ComunicaciÃ³n
Ambos servicios estÃ¡n conectados a travÃ©s de una red Docker llamada mynetwork.
El backend depende del servicio Ollama, asegurando el orden de inicio adecuado.

ğŸš€ InstalaciÃ³n y EjecuciÃ³n
Sigue estos pasos para poner en marcha la aplicaciÃ³n en tu sistema:

1ï¸âƒ£ Clonar el Repositorio
git clone https://github.com/tuusuario/IA-Chat-Gemma2.git
cd IA-Chat-Gemma2

2ï¸âƒ£ Ejecutar los Contenedores con Docker Compose
docker-compose up -d

3ï¸âƒ£ Acceder a la AplicaciÃ³n
Abre tu navegador y dirÃ­gete a:
ğŸ”— http://localhost:7860

ğŸ’¡ CÃ³mo Funciona
1ï¸âƒ£ El servicio Ollama se inicia y descarga el modelo Gemma2 automÃ¡ticamente.
2ï¸âƒ£ La API backend toma las consultas del usuario y las envÃ­a a Ollama.
3ï¸âƒ£ Ollama procesa la consulta y devuelve una respuesta generada por IA.
4ï¸âƒ£ La respuesta se muestra en la interfaz web de Gradio en tiempo real.

ğŸ› ï¸ ResoluciÃ³n de Problemas
ğŸ”´ Problema: "No se puede descargar el modelo"
âœ”ï¸ Verifica tu conexiÃ³n a internet y el espacio disponible en el disco.

ğŸ”´ Problema: "La interfaz web no carga"
âœ”ï¸ AsegÃºrate de que ambos contenedores estÃ¡n en ejecuciÃ³n con docker ps.

ğŸ”´ Problema: "No se puede conectar a Ollama"
âœ”ï¸ Reinicia el servicio con:
      docker-compose restart

ğŸ“œ Licencia
Este proyecto es de cÃ³digo abierto y puede utilizarse con fines educativos y personales.
